{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Load Dataset\n",
    "data=pd.read_csv('./seeds.csv')\n",
    "data.head()\n",
    "\n",
    "data.columns=['Area', 'Perimeter', 'Compactness', 'Kernel_Length','Kernel_Width', 'Asymmetry_Coeff','Kernel_Groove' , 'Type']\n",
    "data.head()\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "data.Type.value_counts()\n",
    "\n",
    "#Dataset Info\n",
    "data.info()\n",
    "\n",
    "#Tukey Method\n",
    "\n",
    "# import required libraries\n",
    "from collections import Counter\n",
    "\n",
    "# Outlier detection \n",
    "def detect_outliers(df,n,features):\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# List of Outliers\n",
    "Outliers_to_drop = detect_outliers(data.drop('Type',axis=1),0,list(data.drop('Type',axis=1)))\n",
    "data.drop('Type',axis=1).loc[Outliers_to_drop]\n",
    "\n",
    "#Create New Dataset without Outliers\n",
    "good_data = data.drop(data.index[Outliers_to_drop]).reset_index(drop = True)\n",
    "good_data.info()\n",
    "\n",
    "#Show Key Statistics\n",
    "good_data.describe()\n",
    "\n",
    "#Create Profile Report\n",
    "    \n",
    "#Importing package\n",
    "import pandas_profiling as pp\n",
    "from IPython.display import IFrame\n",
    " \n",
    "# Profile Report\n",
    "FinalReport = pp.ProfileReport(good_data)\n",
    "FinalReport.to_file('ReportFinalProjChannel-W22.html')\n",
    "display(IFrame('ReportFinalProjChannel-W22.html', width=900, height=350))\n",
    "\n",
    "good_data[\"Category\"].replace({1 :\"Kama\" , 2 :\"Rosa\" , 3 :\"Canadian\"} , inplace = True)\n",
    "good_data.head()\n",
    "\n",
    "#Create x and y variables\n",
    "x = good_data.drop('Type', axis=1).to_numpy()\n",
    "Y = good_data['Type'].to_numpy()\n",
    "\n",
    "#Create Train and Test Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,Y,test_size = 0.2,stratify=Y,random_state = 100)\n",
    "\n",
    "#Scale the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_trainfs = sc.fit_transform(x_train)\n",
    "x_testfs = sc.transform(x_test)\n",
    "\n",
    "#Feature Selection using SelectFromModel\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs',class_weight='balanced',max_iter=1000,random_state=100)\n",
    "clf.fit(x_testfs,y_test)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "feature_idx = model.get_support()\n",
    "feature_name = good_data.drop('Type',axis=1).columns[feature_idx]\n",
    "print('\\nKey Features:',feature_name)\n",
    "\n",
    "#Prepare for Models for Comparison\n",
    "\n",
    "#Create x and y variables\n",
    "x2 = good_data[feature_name].to_numpy()\n",
    "Y2 = good_data['Type'].to_numpy()\n",
    "\n",
    "#Create Train and Test Datasets (New Dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train2,x_test2,y_train2,y_test2 = train_test_split(x2,Y2,test_size = 0.2,stratify=Y2,random_state = 100)\n",
    "\n",
    "#Fix the imbalanced Classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smt=SMOTE(random_state=100)\n",
    "x_train_smt,y_train_smt = smt.fit_resample(x_train2,y_train2)\n",
    "\n",
    "#Scale the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train3 = sc.fit_transform(x_train_smt)\n",
    "x_test3 = sc.transform(x_test2)\n",
    "\n",
    "#Scale the All x-variables to be used with Voting Ensemble\n",
    "\n",
    "x_2 = sc.transform(x2)\n",
    "\n",
    "#Class Balance - Test Data\n",
    "print('Train Data - Class Split')\n",
    "num_zeros = (y_train_smt == \"Kama\").sum()\n",
    "num_ones = (y_train_smt == \"Rosa\").sum()\n",
    "num_two = (y_train_smt == \"Canadian\").sum()\n",
    "print('\"Kama\" -',  num_zeros)\n",
    "print('\"Rosa\" -',  num_ones)\n",
    "print('\"Canadian\" -',  num_two)\n",
    "\n",
    "#Construct some pipelines \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create Pipeline\n",
    "\n",
    "pipeline =[]\n",
    "\n",
    "pipe_logreg = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', LogisticRegression(solver='lbfgs',class_weight='balanced',\n",
    "                                               random_state=100))])\n",
    "pipeline.insert(0,pipe_logreg)\n",
    "\n",
    "pipe_rdf = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', RandomForestClassifier(n_estimators=100,random_state=100))])\n",
    "pipeline.insert(1,pipe_rdf)\n",
    "\n",
    "# Set grid search params \n",
    "\n",
    "modelpara =[]\n",
    "\n",
    "param_gridlogreg = {'clf__C': [0.01, 0.1, 1, 10, 100], \n",
    "                    'clf__penalty': ['l1', 'l2']}\n",
    "modelpara.insert(0,param_gridlogreg)\n",
    "\n",
    "\n",
    "param_gridrdf = {\n",
    "            'clf__n_estimators': [100,150,200],\n",
    "            'clf__max_features': ['auto', 'log2', 'sqrt'],\n",
    "            'clf__bootstrap': [True, False]}\n",
    "modelpara.insert(1,param_gridrdf)\n",
    "\n",
    "#Define Plot for learning curve\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(model):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator=model,\n",
    "                                                            X=x_train_smt, \n",
    "                                                            y=y_train_smt,\n",
    "                                                            train_sizes= np.linspace(0.1, 1.0, 10),\n",
    "                                                            cv=10,\n",
    "                                                            scoring='accuracy',random_state=100)\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.plot(train_sizes, train_mean,color='blue', marker='o', \n",
    "             markersize=5, label='training accuracy')\n",
    "    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std,\n",
    "                     alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5,\n",
    "             label='validation accuracy')\n",
    "    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std,\n",
    "                     alpha=0.15, color='green')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of training samples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim([0.5, 1.01])\n",
    "    plt.show()\n",
    "\n",
    "#Plot Learning Curve\n",
    "print('Logistic Regression - Learning Curve')\n",
    "plot_learning_curves(pipe_logreg)\n",
    "print('\\nRandom Forest - Learning Curve')\n",
    "plot_learning_curves(pipe_rdf)\n",
    "\n",
    "#Model Analysis\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models=[]\n",
    "models.append(('Logistic Regression',pipe_logreg))\n",
    "models.append(('Random Forest',pipe_rdf))\n",
    "\n",
    "#Model Evaluation\n",
    "results =[]\n",
    "names=[]\n",
    "scoring ='accuracy'\n",
    "print('Model Evaluation - Accuracy Score')\n",
    "for name, model in models:\n",
    "    rkf=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)\n",
    "    cv_results = cross_val_score(model,x2,Y2,cv=rkf,scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('{} {:.2f} +/- {:.2f}'.format(name,cv_results.mean(),cv_results.std()))\n",
    "print('\\n')\n",
    "\n",
    "#Boxpot View\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "fig.suptitle('Boxplot View')\n",
    "ax = fig.add_subplot(111)\n",
    "sns.boxplot(data=results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.show()\n",
    "\n",
    "#Define Gridsearch Function\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "def Gridsearch_cv(model, params):\n",
    "    \n",
    "    #Cross-validation Function\n",
    "    cv2=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)\n",
    "        \n",
    "        \n",
    "    #GridSearch CV\n",
    "    gs_clf = GridSearchCV(model, params, cv=cv2,scoring='accuracy')\n",
    "    gs_clf = gs_clf.fit(x_train3, y_train_smt)\n",
    "    model = gs_clf.best_estimator_\n",
    "    \n",
    "    # Use best model and test data for final evaluation\n",
    "    y_pred = model.predict(x_test3)\n",
    "    #Identify Best Parameters to Optimize the Model\n",
    "    bestpara=str(gs_clf.best_params_)\n",
    "    \n",
    "    #Output Heading\n",
    "    print('\\nOptimized Model')\n",
    "    print('\\nModel Name:',str(pipeline.named_steps['clf']))\n",
    "        \n",
    "    #Output Validation Statistics\n",
    "    target_names=['Outcome 0','Outcome 1','Outcome 2']\n",
    "    print('\\nBest Parameters:',bestpara)\n",
    "    print('\\n', confusion_matrix(y_test2,y_pred))  \n",
    "    print('\\n',classification_report(y_test2,y_pred,target_names=target_names)) \n",
    "\n",
    "#Run Models\n",
    "\n",
    "for pipeline, modelpara in zip(pipeline,modelpara):\n",
    "    Gridsearch_cv(pipeline,modelpara)\n",
    "\n",
    "final_model = RandomForestClassifier(random_state=100, bootstrap= True, max_features= 'auto', n_estimators= 100)\n",
    "final_model.fit(x_train3, y_train_smt)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Making predictions\n",
    "y_pred_final = final_model.predict(x_test3)\n",
    "\n",
    "# Printing classification report\n",
    "print('\\n', confusion_matrix(y_test2,y_pred_final))\n",
    "print('\\n', classification_report(y_test2,y_pred_final))\n",
    "\n",
    "# Create a Pickle file  \n",
    "import pickle\n",
    "pickle_out = open(\"model.pkl\",\"wb\")\n",
    "pickle.dump(final_model, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# end\n",
    "\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from flask import Flask, request\n",
    "# from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# app=Flask(__name__)\n",
    "# pickle_in = open(\"model.pkl\",\"rb\")\n",
    "# model=pickle.load(pickle_in)\n",
    "\n",
    "# @app.route('/')\n",
    "# def home():\n",
    "#     return render_template('index.html')\n",
    "\n",
    "\n",
    "\n",
    "# @app.route('/predict',methods=['POST'])\n",
    "# def predict():\n",
    "#     '''\n",
    "#     For rendering results on HTML GUI\n",
    "#     '''\n",
    "#     int_features = [x for x in request.form.values()]\n",
    "#     final_features = [np.array(int_features)]\n",
    "#     prediction = model.predict(final_features)\n",
    "\n",
    "    \n",
    "#     return render_template('index.html', prediction_text='Wheat Kernal is {}'.format(prediction))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     app.run()\n",
    "\n",
    "# #Prepare Models \n",
    "# from sklearn import model_selection\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# #Model Analysis\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# models=[]\n",
    "# models.append(('Logistic Regression',LogisticRegression(solver='lbfgs',class_weight='balanced',\n",
    "#                                                         random_state=100)))\n",
    "# models.append(('Random Forest',RandomForestClassifier(n_estimators=100,\n",
    "#                                                       random_state=100)))\n",
    "# models.append(('Bagging Classifier',BaggingClassifier(random_state=100)))\n",
    "# models.append(('AdaBoost',AdaBoostClassifier(random_state=100)))\n",
    "# models.append(('GBC',GradientBoostingClassifier(random_state=100)))\n",
    "\n",
    "# #Model Evaluation\n",
    "# results =[]\n",
    "# names=[]\n",
    "# scoring ='accuracy'\n",
    "# print('Model Evaluation - Accuracy')\n",
    "# for name, model in models:\n",
    "#     rkf=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)\n",
    "#     cv_results = cross_val_score(model,x2,Y2,cv=rkf,scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print('{} {:.2f} +/- {:.2f}'.format(name,cv_results.mean(),cv_results.std()))\n",
    "# print('\\n')\n",
    "\n",
    "# #Boxpot View\n",
    "# fig = plt.figure(figsize=(15,10))\n",
    "# fig.suptitle('Boxplot View')\n",
    "# ax = fig.add_subplot(111)\n",
    "# sns.boxplot(data=results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Model')\n",
    "# plt.show()\n",
    "\n",
    "# #Script for Models\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "# models2 ={'Logistic Regression':LogisticRegression(solver='lbfgs',\n",
    "#                                                    class_weight='balanced',random_state=100),\n",
    "#           'Random Forest':RandomForestClassifier(n_estimators=100,random_state=100),\n",
    "#           'Bagging Classifier':BaggingClassifier(random_state=100),\n",
    "#           'AdaBoost':AdaBoostClassifier(random_state=100),\n",
    "#           'GBC':GradientBoostingClassifier(random_state=100)}\n",
    "\n",
    "# for name, model in models2.items():\n",
    "#     model.fit(x_train3,y_train_smt)\n",
    "#     predict = model.predict(x_test3)\n",
    "#     print('\\nEstimator: {}'.format(name)) \n",
    "#     print('\\n',confusion_matrix(y_test2,predict))  \n",
    "#     print(classification_report(y_test2,predict))  \n",
    "\n",
    "# #Create Voting Model - Sklearn\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# estimators = []\n",
    "\n",
    "# model1 = LogisticRegression(solver='lbfgs',class_weight='balanced',\n",
    "#                             random_state=100)\n",
    "# estimators.append(('Logistic', model1))\n",
    "\n",
    "# model2 = RandomForestClassifier(n_estimators=100,random_state=100)\n",
    "# estimators.append(('Random Forest', model2))\n",
    "\n",
    "# voting_clf=VotingClassifier(estimators,voting='soft')\n",
    "\n",
    "# scoring = {'acc': 'accuracy',\n",
    "#            'prec_macro': 'precision_macro',\n",
    "#            'rec_macro': 'recall_macro'}\n",
    "# print('\\nVoting Model')\n",
    "# for clf in (model1,model2,voting_clf):\n",
    "#     rkfcv= clf.fit(x_train3,y_train_smt)\n",
    "#     ens_rkf1 = RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)\n",
    "#     rKFcv = cross_validate(rkfcv, x_2, Y2, scoring=scoring, cv=ens_rkf1)\n",
    "#     print(clf.__class__.__name__,round(rKFcv['test_rec_macro'].mean(),2))   \n",
    "\n",
    "# #Create Stacking Model-Sklearn\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# #Identify Models\n",
    "# lr = LogisticRegression(solver='lbfgs',class_weight='balanced',\n",
    "#                         random_state=100)\n",
    "\n",
    "# estimators2 = []\n",
    "\n",
    "# mod1 = RandomForestClassifier(n_estimators=100,random_state=100)\n",
    "# estimators2.append(('Random Forest', mod1))\n",
    "\n",
    "# mod2 = BaggingClassifier(random_state=100)\n",
    "# estimators2.append(('Bagging', mod2))\n",
    "\n",
    "# #Create Stacking Classifier\n",
    "# stackmod=StackingClassifier(estimators=estimators2,\n",
    "#                              final_estimator=lr)\n",
    "\n",
    "# scoring2 = {'acc': 'accuracy',\n",
    "#            'prec_macro': 'precision_macro',\n",
    "#            'rec_macro': 'recall_macro'}\n",
    "\n",
    "# print('\\nStacking Model')\n",
    "# for clf in (mod1,mod2,stackmod):\n",
    "#     rkfcv2= clf.fit(x_train3,y_train_smt)\n",
    "#     ens_rkf2 = RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)\n",
    "#     rKFcv2 = cross_validate(rkfcv2, x_2, Y2, scoring=scoring2, cv=ens_rkf2)\n",
    "#     print(clf.__class__.__name__,round(rKFcv2['test_rec_macro'].mean(),2))  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
